# Basic NN
## vanilla expeirments
- 1 layer of 512 for 1k epochs, batch size 16, image size 244, 244, 3
- 10 layer of 512 for 1k epochs, batch size 16, image size 244, 244, 3
- 20 layers of 512, for 1k epochs, batch size 16, image size 244, 244, 3
- 100 layers of 512, for 1k epochs, batch size 16, image size 244, 244, 3

## dropout
- 1 layer of 512 for 1k epochs, batch size 16, image size 244, 244, 3, dropout 0.3
- 10 layer of 512 for 1k epochs, batch size 16, image size 244, 244, 3, dropout 0.3
- 20 layers of 512, for 1k epochs, batch size 16, image size 244, 244, 3, dropout 0.3
- 100 layers of 512, for 1k epochs, batch size 16, image size 244, 244, 3, dropout 0.3

## data augmentation 
- 1 layer of 512 for 1k epochs, batch size 16, image size 244, 244, 3, dropout 0.3, data augmentation layer
- 10 layer of 512 for 1k epochs, batch size 16, image size 244, 244, 3, dropout 0.3,  data augmentation layer
- 20 layers of 512, for 1k epochs, batch size 16, image size 244, 244, 3, dropout 0.3,  data augmentation layer
- 100 layers of 512, for 1k epochs, batch size 16, image size 244, 244, 3, dropout 0.3,  data augmentation layer

# CNN 

# Resnet Transfer Learning

# Xception Net Transfer Learning


# Efficient Net Transfer Learning

## data augmentation added
- keras efficient net b0, 1k epochs batch size 16, data augmentation layer (18s/epoch => 5hrs training time)
- keras efficient net b7, 1k epochs batch size 16, data augmentation layer

